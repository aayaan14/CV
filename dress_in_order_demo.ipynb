{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ddd03df8e2c041cabbf975c5351a751f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6acfcb8ba6847298e862ba8537885bc",
              "IPY_MODEL_69f91efa23d44bf59be393e726489554",
              "IPY_MODEL_0b00f3cdbbd9415a8209dff2638566d1"
            ],
            "layout": "IPY_MODEL_eb7a84a0e6b84978b877201d28dd2c15"
          }
        },
        "e6acfcb8ba6847298e862ba8537885bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d65cfc1fd6411ba8c7a50b3304976d",
            "placeholder": "​",
            "style": "IPY_MODEL_6669bbe83290416ea3aa59606e8d6f75",
            "value": "100%"
          }
        },
        "69f91efa23d44bf59be393e726489554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d39f1c71ee57426380a3b624531eb51a",
            "max": 44096,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b27dded10f44a6fb5db0aeb641f9760",
            "value": 44096
          }
        },
        "0b00f3cdbbd9415a8209dff2638566d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddaf5f189b6f4b8fa14a9f273e594cf5",
            "placeholder": "​",
            "style": "IPY_MODEL_6b1d1c75707040eeb2eba2908a703c64",
            "value": " 44096/44096 [00:07&lt;00:00, 5996.25it/s]"
          }
        },
        "eb7a84a0e6b84978b877201d28dd2c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d65cfc1fd6411ba8c7a50b3304976d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6669bbe83290416ea3aa59606e8d6f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d39f1c71ee57426380a3b624531eb51a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b27dded10f44a6fb5db0aeb641f9760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddaf5f189b6f4b8fa14a9f273e594cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1d1c75707040eeb2eba2908a703c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayaan14/CV/blob/main/dress_in_order_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The __Official__ Colab Demo for ICCV'21 paper\n",
        "\n",
        "__[Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-on and Outfit Editing](https://arxiv.org/abs/2104.07021)__\n",
        "\n",
        "\n",
        "\\[[Code](https://github.com/cuiaiyu/dressing-in-order)\\]\n",
        "\\[[Paper](https://arxiv.org/abs/2104.07021)\\]\n",
        "\n",
        "# Read Before Starting\n",
        "- This Colab Demo is available for __non-commercial research purposes__ only. \n",
        "- This Colab contains data downloading scripts. __*Please make sure you are legally allowed to use the [DeepFahsion-MultiModal dataset](https://github.com/yumingj/DeepFashion-MultiModal) as required by [their license]() before trying this demo.*__\n"
      ],
      "metadata": {
        "id": "uxcLBG9aa54D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Install Environment\n",
        "(This demo only supports inference, so we don't install GFLA.)"
      ],
      "metadata": {
        "id": "AFP7Kc8HGnb4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrc6NTjyUDhs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check GPU \n",
        "# If you don't have GPU, please set it by Runtime -> Change runtime type\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxLGbnKdGwxP",
        "outputId": "32a51082-2b60-4391-811e-29a6c03d2c8e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 18 13:04:59 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEaYleTmGNiT",
        "outputId": "2398ad3d-64e2-4c69-870c-deac6ae10f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dressing-in-order'...\n",
            "remote: Enumerating objects: 201, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 201 (delta 83), reused 183 (delta 75), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (201/201), 5.13 MiB | 22.27 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n"
          ]
        }
      ],
      "source": [
        "# install DiOr\n",
        "import os\n",
        "!git clone https://github.com/cuiaiyu/dressing-in-order\n",
        "repo_name='dressing-in-order'\n",
        "os.chdir(f'./{repo_name}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-image\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoatprfDIh08",
        "outputId": "0d20a581-640c-4a44-9b26-457d27b00aab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.5.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Download Data\n",
        "This Data is from [DeepFashion-Multimodal](https://arxiv.org/abs/2205.15996) [1].\n",
        "\n",
        "Please make sure you read [the dataset license](https://github.com/yumingj/DeepFashion-MultiModal/blob/main/LICENSE) before running the below code. \n",
        "\n",
        "You are __responsible__ to make sure whether the data is available for your usage.\n",
        "\n",
        "```[1] Jiang, Yuming, et al. \"Text2human: Text-driven controllable human image generation.\" ACM Transactions on Graphics (TOG) 41.4 (2022): 1-11.```\n",
        "\n"
      ],
      "metadata": {
        "id": "M9q_yUPNKTBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data from https://github.com/yumingj/DeepFashion-MultiModal\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if not os.path.exists(\"data/\"):\n",
        "  os.system(\"mkdir data\")\n",
        "\n",
        "def download_from_gdrive(dst_root, fn, gdrive_path, iszip=True):\n",
        "  if not os.path.exists(dst_root):\n",
        "    os.system(\"mkdir {}\".format(dst_root))\n",
        "  if not os.path.exists(\"{}/{}\".format(dst_root, fn)):\n",
        "    os.system(\"gdown {}\".format(gdrive_path))\n",
        "    if iszip:\n",
        "      os.system(\"unzip {}.zip\".format(fn))\n",
        "      os.system(\"rm {}.zip\".format(fn))\n",
        "    os.system(\"mv {} {}/\".format(fn, dst_root))\n",
        "  print(\"download {}.\".format(fn))\n",
        "\n",
        "# download data\n",
        "download_from_gdrive(\"data\", \"testM_lip\", \"1toeQwAe57LNPTy9EWGG0u1XfTI7qv6b1\")\n",
        "download_from_gdrive(\"data\", \"images\", \"1U2PljA7NE57jcSSzPs21ZurdIPXdYZtN\")\n",
        "download_from_gdrive(\"data\",\"fasion-pairs-test.csv\",\"12fZKGf0kIu5OX3mjC-C3tptxrD8sxm7x\",iszip=False)\n",
        "download_from_gdrive(\"data\",\"fasion-annotation-test.csv\",\"1MxkVFFtNsWFshQp_TA7qwIGEUEUIpYdS\",iszip=False)\n",
        "download_from_gdrive(\"data\",\"standard_test_anns.txt\",\"19nJSHrQuoJZ-6cSl3WEYlhQv6ZsAYG-X\",iszip=False)\n",
        "\n",
        "# filter images (exclude training data and rename the files)\n",
        "if not os.path.exists(\"data/test\"):\n",
        "  os.mkdir(\"data/test\")\n",
        "target_fns = [fn[:-4] for fn in os.listdir(\"data/testM_lip\")]\n",
        "for fn in tqdm(os.listdir(\"data/images\")):\n",
        "  elements = fn.split(\"-\")\n",
        "  elements[2] = elements[2].replace(\"_\",\"\")\n",
        "  last_elements = elements[-1].split(\"_\")\n",
        "  elements[-1] = last_elements[0] + \"_\" + last_elements[1] + last_elements[2]\n",
        "  new_fn = \"fashion\"+\"\".join(elements)\n",
        "\n",
        "  if new_fn[:-4] in target_fns:\n",
        "    os.system(\"mv {} {}\".format(\"data/images/\"+fn, \"data/test/\"+new_fn))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "ddd03df8e2c041cabbf975c5351a751f",
            "e6acfcb8ba6847298e862ba8537885bc",
            "69f91efa23d44bf59be393e726489554",
            "0b00f3cdbbd9415a8209dff2638566d1",
            "eb7a84a0e6b84978b877201d28dd2c15",
            "d5d65cfc1fd6411ba8c7a50b3304976d",
            "6669bbe83290416ea3aa59606e8d6f75",
            "d39f1c71ee57426380a3b624531eb51a",
            "6b27dded10f44a6fb5db0aeb641f9760",
            "ddaf5f189b6f4b8fa14a9f273e594cf5",
            "6b1d1c75707040eeb2eba2908a703c64"
          ]
        },
        "id": "q8hcmJInL0ho",
        "outputId": "e99556c1-1231-4345-de26-a2959ee087d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download testM_lip.\n",
            "download images.\n",
            "download fasion-pairs-test.csv.\n",
            "download fasion-annotation-test.csv.\n",
            "download standard_test_anns.txt.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/44096 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddd03df8e2c041cabbf975c5351a751f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load Pre-trained Model"
      ],
      "metadata": {
        "id": "R-CZ6KMscZzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.dior_model import DIORModel\n",
        "import os, json\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "fOFBTL6sX1tP",
        "outputId": "025fddd8-7640-46ab-9056-e38c838166f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bf0473eeaac3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdior_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDIORModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dressing-in-order/dressing-in-order/dressing-in-order/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dressing-in-order/dressing-in-order/dressing-in-order/models/base_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dressing-in-order/dressing-in-order/dressing-in-order/models/networks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_networks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgfla\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoseFlowNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResDiscriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dressing-in-order/dressing-in-order/dressing-in-order/models/networks/vgg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_optical_flow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlyingChairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlyingThings3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHD1K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKittiFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSintel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from ._stereo_matching import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mCarlaStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mCREStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mETH3DStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/_optical_flow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_png_16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_pfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_str_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_deprecate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_directory' from 'PIL._util' (/usr/local/lib/python3.10/dist-packages/PIL/_util.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = 'data'\n",
        "exp_name = 'DIORv1_64' # DIOR_64\n",
        "epoch = 'latest'\n",
        "netG = 'diorv1' # dior\n",
        "ngf = 64\n",
        "\n",
        "## this is a dummy \"argparse\" \n",
        "class Opt:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "if True:\n",
        "    opt = Opt()\n",
        "    opt.dataroot = dataroot\n",
        "    opt.isTrain = False\n",
        "    opt.phase = 'test'\n",
        "    opt.n_human_parts = 8; opt.n_kpts = 18; opt.style_nc = 64\n",
        "    opt.n_style_blocks = 4; opt.netG = netG; opt.netE = 'adgan'\n",
        "    opt.ngf = ngf\n",
        "    opt.norm_type = 'instance'; opt.relu_type = 'leakyrelu'\n",
        "    opt.init_type = 'orthogonal'; opt.init_gain = 0.02; opt.gpu_ids = [0]\n",
        "    opt.frozen_flownet = True; opt.random_rate = 1; opt.perturb = False; opt.warmup=False\n",
        "    opt.name = exp_name\n",
        "    opt.vgg_path = ''; opt.flownet_path = ''\n",
        "    opt.checkpoints_dir = 'checkpoints'\n",
        "    opt.frozen_enc = True\n",
        "    opt.load_iter = 0\n",
        "    opt.epoch = epoch\n",
        "    opt.verbose = False\n",
        "\n",
        "# create model\n",
        "#os.mkdir(\"checkpoints\")\n",
        "download_from_gdrive(\"checkpoints\", \"DIORv1_64\", \"1MyHq-P0c8zz7ey7p_HTTZKeMie5ZuNlb\")\n",
        "\n",
        "model = DIORModel(opt)\n",
        "model.setup(opt)\n",
        "     "
      ],
      "metadata": {
        "id": "XnAD8DAHHKcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "from datasets.deepfashion_datasets import DFVisualDataset\n",
        "Dataset = DFVisualDataset\n",
        "ds = Dataset(dataroot=dataroot, dim=(256,176), n_human_part=8)\n",
        "\n",
        "# preload a set of pre-selected models defined in \"standard_test_anns.txt\" for quick visualizations \n",
        "inputs = dict()\n",
        "for attr in ds.attr_keys:\n",
        "    inputs[attr] = ds.get_attr_visual_input(attr)\n",
        "    \n",
        "# define some tool functions for I/O\n",
        "def load_img(pid, ds):\n",
        "    if len(pid[0]) < 10: # load pre-selected models\n",
        "        person = inputs[pid[0]]\n",
        "        person = (i.cuda() for i in person)\n",
        "        pimg, parse, to_pose = person\n",
        "        pimg, parse, to_pose = pimg[pid[1]], parse[pid[1]], to_pose[pid[1]]\n",
        "    else: # load model from scratch\n",
        "        person = ds.get_inputs_by_key(pid[0])\n",
        "        person = (i.cuda() for i in person)\n",
        "        pimg, parse, to_pose = person\n",
        "    return pimg.squeeze(), parse.squeeze(), to_pose.squeeze()\n",
        "\n",
        "def plot_img(pimg=[], gimgs=[], oimgs=[], gen_img=[], pose=None):\n",
        "    if pose != None:\n",
        "        import utils.pose_utils as pose_utils\n",
        "        print(pose.size())\n",
        "        kpt = pose_utils.draw_pose_from_map(pose.cpu().numpy().transpose(1,2,0),radius=6)\n",
        "        kpt = kpt[0]\n",
        "    if not isinstance(pimg, list):\n",
        "        pimg = [pimg]\n",
        "    if not isinstance(gen_img, list):\n",
        "        gen_img = [gen_img]\n",
        "    out = pimg + gimgs + oimgs + gen_img\n",
        "    if out:\n",
        "        out = torch.cat(out, 2).float().cpu().detach().numpy()\n",
        "        out = (out + 1) / 2 # denormalize\n",
        "        out = np.transpose(out, [1,2,0])\n",
        "\n",
        "        if pose != None:\n",
        "            out = np.concatenate((kpt, out),1)\n",
        "    else:\n",
        "        out = kpt\n",
        "    fig = plt.figure(figsize=(6,4), dpi= 100, facecolor='w', edgecolor='k')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(out)\n",
        "\n",
        "# define dressing-in-order function (the pipeline)\n",
        "def dress_in_order(model, pid, pose_id=None, gids=[], ogids=[], order=[5,1,3,2], perturb=False):\n",
        "    PID = [0,4,6,7]\n",
        "    GID = [2,5,1,3]\n",
        "    # encode person\n",
        "    pimg, parse, from_pose = load_img(pid, ds)\n",
        "    if perturb:\n",
        "        pimg = perturb_images(pimg[None])[0]\n",
        "    if not pose_id:\n",
        "        to_pose = from_pose\n",
        "    else:\n",
        "        to_img, _, to_pose = load_img(pose_id, ds)\n",
        "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
        "\n",
        "    # encode base garments\n",
        "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
        "   \n",
        "    \n",
        "    # swap base garment if any\n",
        "    gimgs = []\n",
        "    for gid in gids:\n",
        "        _,_,k = gid\n",
        "        gimg, gparse, pose =  load_img(gid, ds)\n",
        "        seg = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=gid[2])\n",
        "        gsegs[gid[2]] = seg\n",
        "        gimgs += [gimg * (gparse == gid[2])]\n",
        "\n",
        "    # encode garment (overlay)\n",
        "    garments = []\n",
        "    over_gsegs = []\n",
        "    oimgs = []\n",
        "    for gid in ogids:\n",
        "        oimg, oparse, pose = load_img(gid, ds)\n",
        "        oimgs += [oimg * (oparse == gid[2])]\n",
        "        seg = model.encode_single_attr(oimg[None], oparse[None], pose[None], to_pose[None], i=gid[2])\n",
        "        over_gsegs += [seg]\n",
        "    \n",
        "    gsegs = [gsegs[i] for i in order] + over_gsegs\n",
        "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
        "    \n",
        "    return pimg, gimgs, oimgs, gen_img[0], to_pose\n",
        "     \n"
      ],
      "metadata": {
        "id": "eW-7_qnVYOky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "NOTE: INDEX OF GARMENT is used as following:\n",
        "\n",
        "- 'top':5, # dress is also considered as top.\n",
        "- 'bottom':1,\n",
        "- 'hair':2,\n",
        "- 'jacket':3\n"
      ],
      "metadata": {
        "id": "xKY9RFe9icn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pose Transfer"
      ],
      "metadata": {
        "id": "l_GE-cGraZxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# person id\n",
        "pid = (\"print\", 0, None) # load the 0-th person from \"print\" group, NONE (no) garment is interested\n",
        "# pose id (take this person's pose)\n",
        "pose_id = (\"print\", 2, None) # load the 2-nd person from \"print\" group, NONE (no) garment is interested\n",
        "# generate\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, pose_id=pose_id)\n",
        "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
      ],
      "metadata": {
        "id": "Taih27BjKSNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Virtual Try-On (Tucking in/out)\n",
        "\n",
        "Users can control the tuck-in and tuck-out results when there is overlap between the top garment and the bottom garment."
      ],
      "metadata": {
        "id": "Krq8Rf4vaeNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pid = (\"pattern\", 3, None) # load the 3-rd person from \"pattern\" group, NONE (no) garment is interested\n",
        "gids = [\n",
        "   (\"plaid\",0,5), # load the 0-th person from \"plaid\" group, garment #5 (top) is interested\n",
        "   (\"pattern\",3,1),  # load the 3-rd person from \"pattern\" group, garment #1 (bottom) is interested\n",
        "       ]\n",
        "\n",
        "# tuck in (dressing order: hair, top, bottom)\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, gids=gids, order=[2,5,1])\n",
        "plot_img(pimg, gimgs, gen_img=gen_img, pose=pose)\n",
        "\n",
        "# not tuckin (dressing order: hair, bottom, top)\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, gids=gids, order=[2,1,5])\n",
        "plot_img(pimg, gimgs, gen_img=gen_img, pose=pose)\n",
        "     \n",
        "  "
      ],
      "metadata": {
        "id": "bOyBfIsyHTTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Virtual Try-On (Layering)\n"
      ],
      "metadata": {
        "id": "k4QfUWboazWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pid = ('plaid',3, 5)\n",
        "ogids = [('print', 2, 5)]\n",
        "# tuck in\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid, ogids=ogids)\n",
        "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
      ],
      "metadata": {
        "id": "HLbUttklH3OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Virual Try-On (Layering - Muliple)\n"
      ],
      "metadata": {
        "id": "NetiYTbTgRJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# person id\n",
        "pid = (\"fashionWOMENBlouses_Shirtsid0000637003_1front.jpg\", None , None) # load person from the file\n",
        "\n",
        "# garments to try on (ordered)\n",
        "gids = [\n",
        "    (\"gfla\",2,5),\n",
        "    (\"strip\",3,1),\n",
        "       ]\n",
        "\n",
        "# garments to lay over (ordered)\n",
        "ogids = [\n",
        " (\"fashionWOMENTees_Tanksid0000159006_1front.jpg\", None ,5),\n",
        " ('fashionWOMENJackets_Coatsid0000645302_1front.jpg', None ,3),    \n",
        "]\n",
        "\n",
        "# dressing in order\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order(model, pid=pid, gids=gids, ogids=ogids)\n",
        "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
      ],
      "metadata": {
        "id": "KuMRvu9ceZvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Opacity"
      ],
      "metadata": {
        "id": "TnF5FnQBeXRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dress_in_order_opcaity(model, pid, pose_id=None, gids=[], ogids=[], order=[5,1,3,2]):\n",
        "    PID = [0,4,6,7]\n",
        "    # encode person\n",
        "    pimg, parse, from_pose = load_img(pid, ds)\n",
        "    if not pose_id:\n",
        "        to_pose = from_pose\n",
        "    else:\n",
        "        to_img, _, to_pose = load_img(pose_id, ds)\n",
        "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
        "\n",
        "    # encode base garments\n",
        "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
        "\n",
        "    # swap base garment if any\n",
        "    gimgs = []\n",
        "    for gid in gids:\n",
        "        gimg, gparse, pose =  load_img(gid, ds)\n",
        "        seg = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=gid[2])\n",
        "        gsegs[gid[2]] = seg\n",
        "        gimgs += [gimg]\n",
        "\n",
        "    # encode garment (overlay)\n",
        "    garments = []\n",
        "    over_gsegs = []\n",
        "    oimgs = []\n",
        "    for gid in ogids:\n",
        "        oimg, oparse, pose = load_img(gid, ds)\n",
        "        oimgs.append(oimg)\n",
        "        seg = model.encode_single_attr(oimg[None], oparse[None], pose[None], to_pose[None], i=gid[2])\n",
        "        over_gsegs += [seg]\n",
        "        \n",
        "    gsegs = [gsegs[i] for i in order] + over_gsegs\n",
        "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
        "    \n",
        "    gmap, mask = gsegs[-1]\n",
        "    gens = []\n",
        "    for alpha in [0, 0.99, 0.6, 0.35, 0.25]:\n",
        "        curr_mask = mask.clone() # * alpha\n",
        "        curr_mask[curr_mask >= alpha] = alpha\n",
        "        gsegs[-1] = gmap, curr_mask\n",
        "        img = model.netG(to_pose[None], psegs, gsegs, alpha=0.1999)\n",
        "        gens.append(img[0])\n",
        "    return pimg, gimgs, oimgs, gens, to_pose\n",
        "\n",
        "pid = ('fashionWOMENBlouses_Shirtsid0000270306_1front.jpg', None, 5)\n",
        "gids = [('fashionWOMENTees_Tanksid0000255303_1front.jpg', None, 5)]\n",
        "ogids = [('fashionWOMENBlouses_Shirtsid0000270306_1front.jpg', None, 5)]\n",
        "\n",
        "pimg, gimgs, oimgs, gens, to_pose = dress_in_order_opcaity(model, pid=pid, gids=gids, ogids=ogids)\n",
        "\n",
        "# plot results: (source person, source garment-inside, transparency results)\n",
        "output = torch.cat([pimg, gimgs[0],gens[0]] + gens[::-1][:-1], 2)\n",
        "output = (output + 1) / 2\n",
        "output = output.float().cpu().detach().numpy()\n",
        "output = np.transpose(output, [1,2,0])\n",
        "fig=plt.figure(figsize=(6,4), dpi= 200, facecolor='w', edgecolor='k')\n",
        "plt.imshow(output)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "G0yPTUqQaDA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping\n"
      ],
      "metadata": {
        "id": "djcD8QndgZDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dress_in_order_texshape(model, pid, target=5, shape_id=None, tex_patch=None, order=[2,5,1,3]):\n",
        "    PID = [0,4,6,7]\n",
        "    # encode person\n",
        "    pimg, parse, from_pose = load_img(pid, ds)\n",
        "    to_pose = from_pose\n",
        "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
        "\n",
        "    # encode base garments\n",
        "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
        "    \n",
        "    fmap, mask = gsegs[target]\n",
        "    gimg = pimg*(parse==target)\n",
        "    if shape_id != None:\n",
        "        gimg, gparse, pose =  load_img(shape_id, ds)\n",
        "        _, mask = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=target)\n",
        "        shape_img = [gimg*(gparse==target)]\n",
        "    else:\n",
        "        shape_img = []\n",
        "    if tex_patch != None:\n",
        "        fmap = model.netE_attr(tex_patch, model.netVGG)\n",
        "    gsegs[target] = fmap, mask\n",
        "    gsegs = [gsegs[i] for i in order]\n",
        "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
        "    \n",
        "    return pimg, [gimg], shape_img, gen_img[0], to_pose"
      ],
      "metadata": {
        "id": "Gnf7lQTfgb4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pid = ('print', 0, 5) \n",
        "shape_id = ('plain', 2, 5) \n",
        "\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order_texshape(model, pid, shape_id=shape_id)\n",
        "plot_img(pimg, gimgs, oimgs, gen_img, pose)"
      ],
      "metadata": {
        "id": "TsRDArEwghnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texture Transfer"
      ],
      "metadata": {
        "id": "FI-it6f4g1kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pid = ('print', 0, 5) \n",
        "patch_id = ('plaid', 0, 5) \n",
        "patch, parse, from_pose = load_img(patch_id, ds)\n",
        "pimg, gimgs, oimgs, gen_img, pose = dress_in_order_texshape(model, pid, tex_patch=patch[None])\n",
        "plot_img(pimg, [patch], oimgs, gen_img, pose)\n",
        "     "
      ],
      "metadata": {
        "id": "D5IwOnnZgsrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Insertion (RGBA image required)\n"
      ],
      "metadata": {
        "id": "-zlQCwn6hGgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download from https://www.stickpng.com/img/memes/doge/doge-facing-right\n",
        "import cv2\n",
        "download_from_gdrive(\"data\",\"doge.png\",\"1ZjEZVWGyLX7Mefrkc03uaEsspP0Nk_EL\",iszip=False)\n",
        "\n",
        "fn = \"data/doge.png\"\n",
        "pid = ('plain', 3, None)\n",
        "\n",
        "image = cv2.imread(fn, cv2.IMREAD_UNCHANGED) #Read RGBA image\n",
        "# put the print on a blank canvas\n",
        "x,y,h,w = 90,60,80,70\n",
        "image = cv2.resize(image, (w,h))\n",
        "bg = np.zeros((256,176,4))\n",
        "bg[x:x+h,y:y+w] = image\n",
        "image = bg\n",
        "\n",
        "# crop the print image\n",
        "trans_mask = image[:,:,3] != 0\n",
        "image = image[:,:,2::-1].transpose(2,0,1)\n",
        "image = (image / 255.0) * 2 - 1\n",
        "image = image * trans_mask[None]\n",
        "\n",
        "\n",
        "# run DiOr\n",
        "pimg, parse, to_pose =  load_img(pid, ds) \n",
        "psegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [0,4,6,7])\n",
        "gsegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [5,1,2])\n",
        "# insert the print\n",
        "print_image = torch.from_numpy(image).float().cuda()\n",
        "print_fmap = model.netE_attr(print_image[None], model.netVGG)\n",
        "print_mask = model.netE_attr.module.segmentor(print_fmap)\n",
        "gsegs = gsegs[:1] + [(print_fmap, torch.sigmoid(print_mask))] + gsegs[1:] \n",
        "# generate\n",
        "gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
        "\n",
        "# construct a copy-and-paste image for comparison\n",
        "paste_img = image + pimg.cpu().detach().numpy() * (1 - trans_mask[None])\n",
        "paste_img = torch.from_numpy(paste_img).float().cuda()\n",
        "\n",
        "# display\n",
        "output = torch.cat([pimg, paste_img, gen_img[0]],2)\n",
        "output = output.float().cpu().detach().numpy()\n",
        "output = (output + 1) / 2\n",
        "output = np.transpose((output * 255.0).astype(np.uint8), [1,2,0])\n",
        "fig=plt.figure(figsize=(6,4), dpi= 100, facecolor='w', edgecolor='k')\n",
        "plt.imshow(output)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "UYfmqt11hIhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDBSojy3vd2o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}